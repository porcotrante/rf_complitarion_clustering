# Efficient Partitioning-Based Compilation of Random Forests into Decision Trees

This (anonymous) repository contains the Rust implementation accompanying the paper titled "Efficient Partitioning-Based Compilation of Random Forests into Decision Trees". The code demonstrates the proposed partitioning strategy for accelerating the compilation of Random Forest (RF) ensembles into single semantically equivalent Decision Trees (DTs).

## Overview

The core contribution implemented here is a **partitioning-based compilation strategy** designed to significantly speed up existing state-of-the-art RF-to-DT compilation methods (specifically, the approach detailed in [17] of the paper's references). This code allows for benchmarking this strategy against the baseline (non-partitioned, k=0) approach.

The key idea is to recursively:
1.  **Divide:** Split the feature space based on selected feature thresholds.
2.  **Simplify:** Prune the nodes of the forest's trees based on the bounds defined by the current partition.
3.  **Recurse:** Solve the smaller compilation subproblems within each partition (potentially in parallel).
4.  **Assemble:** Combine the results from subproblems into a single decision tree for the current partition level.

## Code Structure

The codebase is organized into several modules within the `src/` directory:

*   **`main.rs`**: The main executable handling configuration, orchestration of benchmarks across seeds/k/strategies, results collection, CSV output, and calling the visualization script.
*   **`config.rs`**: Contains constants for dataset selection, k values, strategies, seeds, etc.
*   **`transform/`**: Contains the core compilation logic.
    *   **`transform.rs`**: Top-level functions `transform_rf` (k=0 baseline) and `transform_rf_partitioned` (k>0 partitioning), including the main recursive partitioning logic (`_transform_rf_partitioned_recursive`) demonstrating arena management and the divide/simplify/recurse/assemble flow.
    *   **`builder.rs`**: Implements the `TreeBuilder` responsible for the sequential merge process used in the baseline (k=0) and within the base cases of the partitioning recursion (k=0 leaves). Includes different early stopping leaf transition handlers.
    *   **`early_stopping.rs`**: Implements the various early stopping strategies (ES, AbsES, HEUR, ORD) described in the paper, including the core abstract interpretation (`abstract_interpret_dt`) and reachable leaves counting (`count_reachable_leaves`) logic.
    *   **`simplify.rs`**: Contains functions for simplifying tree structures based on path conditions (`BoundsMap`), both from initial array representations (`simplify_single_tree`) and from existing arenas within partitions (`resimplify_from_arena`).
    *   **`partition.rs`**: Helpers related to partitioning, notably `_copy_and_intern_nodes` used for assembling results by copying/interning nodes between arenas.
    *   **`common.rs`**: Shared utilities like `BoundsMap` definition and bound checking.
    *   **`data.rs`**: Logic for loading the RF model data from JSON.
    *   **`predict.rs`**: Functions to predict using the compiled tree (`predict_with_merged_tree`) or the original RF structure (`predict_with_original_rf`, used for verification).
*   **`tree.rs`**: Implements the `Arena` allocator using a Structure-of-Arrays layout and hash-consing for efficient node storage and deduplication.
*   **`results.rs`**: Contains the `BenchmarkResult` struct definition and functions for printing the summary table and writing/appending to the CSV output file.
*   **`external.rs`**: Contains the function `run_visualization` for calling the external Python plotting script.
*   **`utils.rs`**: General utility functions (e.g., `calculate_stats`).
*   **`cpu_time.rs`**: Platform-specific CPU time measurement.
*   **`export.rs`**: Functions to export the compiled tree.

## Prerequisites

1.  **Rust:** A stable Rust toolchain is required. Installation instructions: [https://www.rust-lang.org/tools/install](https://www.rust-lang.org/tools/install)
2.  **Python 3:** (Optional) Required for python scripts (visualization, training).
3.  **Python Libraries:** (Optional) Check pyproject.toml

## Data

The benchmark requires pre-trained Random Forest models and corresponding prediction files. These are **not included** in the repository (but can be generated by the train.py script).

*   **Format:**
    *   RF Models: JSON files (e.g., `rf-banknote-seed42.json`) containing the tree structures in an array-based format (compatible with `load_rf_data`).
    *   Predictions: CSV files (e.g., `pred-banknote-seed42.csv`) containing test instances (features) and a column named `rf_pred` with the prediction made by the *original* RF model for verification.
*   **Location:** Place these files inside the `src/rf/` directory relative to the `Cargo.toml` file.
*   **Generation:** These files can be generated by the script train.py. For the datasets in the paper, you can just run `python train.py -d DATASET_NAME` (e.g., `python train.py -d iris`).
*   **Dataset stats**: You can also use the train.py script to get statistics about a dataset. Run `python train.py -d DATASET_NAME --stats`.

## Configuration

Benchmark parameters can be adjusted by modifying the *compile-time* constants at the top of `src/config.rs`:

*   `CHOSEN_CONFIG`: Select the dataset to benchmark (e.g., `BANKNOTE_CONFIG`, `MAGIC_CONFIG`). The corresponding `NUM_FEATURES` and `NUM_CLASSES` constants will automatically match the chosen dataset.
*   `K_VALUES`: An array specifying the partitioning depths (`k`) to test (e.g., `[0, 2, 4, 8]`). `k=0` runs the baseline non-partitioned algorithm.
*   `STRATEGIES_TO_TEST`: An array specifying which early stopping strategies (`EarlyStoppingStrategy::Standard`, `::Abstract`, etc.) to test.
*   `NUM_BENCHMARK_RUNS`: How many times to repeat the compilation for each (seed, k, strategy) configuration to get stable timing metrics.
*   `BASE_SEED`, `NUM_SEEDS_TO_RUN`: Define the range of random seeds for which corresponding RF models and prediction files should exist and be benchmarked.

## Running the Benchmarks

First, go to `src/config.rs` and change the `CHOSEN_CONFIG` variable to your desired dataset (e.g., `IRIS_CONFIG`). You can also change other configurations (number of seeds, number of runs, strategies to test, k values to test, etc.).

Ensure you have already trained the RFs you'll benchmark (use the train.py script) and that `BASE_SEED` and `NUM_SEEDS_TO_RUN` are properly configured in `src/config.rs`.

Then, execute the benchmarks from the root directory of the project (where `Cargo.toml` is located) using Cargo. 

**Use the `--release` flag.** If you don't use the flag, the compiler will generate non-optimized (debug variant) code.

```bash
cargo run --release
```

The program will:
1.  Load the RF model and prediction file for each configured seed.
2.  Run the compilation (`transform_rf_partitioned`) for each configured `k` value and strategy, repeated `NUM_BENCHMARK_RUNS` times.
3.  Measure wall time, CPU time, and component timings.
4.  Verify the compiled tree's predictions against the `rf_pred` column in the prediction CSV.
5.  Append the aggregated results for the current seed to the results CSV file.
6.  Print progress and results to the console.
7.  After processing all seeds, print a final summary table.
8.  Attempt to run the Python visualization script.

## Output

1.  **Console Output:** Shows configuration details, progress messages for each seed/k/strategy, timing results, mismatch counts, final node counts/height, and a summary table at the end. Mismatches during verification are logged for inspection.
2.  **CSV File:** Results are incrementally appended to `results/benchmark_results_DATASET_all_seeds.csv` (where `DATASET` is the configured dataset name). This file contains detailed timing metrics (min, median, max, mean, stddev for wall/CPU/components), node count, height, mismatches, and configuration details (seed, k, strategy). If the `detailed-stats` feature is enabled during compilation (`cargo run --release --features detailed-stats`), additional columns for pruning counts will be included. Running with detailed-stats will make the RF-to-DF compilation slower, so if you are interested in benchmarking the speedup, do not use the feature detailed-stats (i.e., just run `cargo run --release`).
3.  **Plot (Optional):** If the Python visualization script runs successfully, it should generate many plots in a folder `results/DATASET`.

## Verification

The correctness of the compilation (functional equivalence) is verified by comparing the predictions of the final compiled tree (`predict_with_merged_tree`) against the pre-computed predictions of the original Random Forest (`rf_pred` column loaded from the `pred-*.csv` file) for each instance in the prediction file. Any discrepancies are reported as mismatches.

## Implementation Details

*   **Arena & Hash-Consing:** See `src/tree.rs` for the Structure-of-Arrays based arena implementation enabling node deduplication.
*   **Partitioning Logic:** The core recursive strategy is in `src/transform/transform.rs` (`_transform_rf_partitioned_recursive`). Note the management of arena ownership and the simplify/recurse/assemble steps.
*   **Simplification:** `src/transform/simplify.rs` shows how path conditions (`BoundsMap`) are used to prune nodes during `resimplify_from_arena` (within partitions) and `simplify_single_tree` (initial conversion).
*   **Baseline Merge:** `src/transform/builder.rs` contains the `TreeBuilder` logic implementing the sequential merge process from [17].
*   **Early Stopping & Abstract Interpretation:** `src/transform/early_stopping.rs` implements the different strategies and the underlying abstract interpretation logic.
*   **Node Copying (Assembly):** `src/transform/partition.rs` (`_copy_and_intern_nodes`) is essential for assembling results from different partitions/arenas back together while maintaining deduplication.

## Sequential execution

You can use the feature `non-parallel` for sequential execution in the partitioned case ($k > 0$).

Example: `cargo run --release --features non-parallel`

## Troubleshooting

- If you run into a stack overflow error, the forest is probably too big and the amount of recursive calls surpassed the maximum allowed. Check if .cargo/config.toml is properly setup with 8MB for the stack (Windows default is 1MB, and we changed it to 8MB). It should be already setup if you cloned this repository. Increasing the stack size might help. Improving the memory allocation (to use more of the heap, and less of the stack) might help as well.
- If you run into Windows error code 5 (0x00000005) "ACCESS DENIED" (or similar in other OSes), this is probably a stack overflow (see previous point).
- If you use another dataset (one not handled by `train.py`), ensure to properly setup `src/config.rs`. You need to pass the number of features and classes in compile-time. Example: for dataset "name" with 4 features and 2 classes: `const CUSTOM_CONFIG: (&str, usize, usize) = ("name", 4, 2);` and then update `CHOSEN_CONFIG` to point to your custom config.